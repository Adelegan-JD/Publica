{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4272ce",
   "metadata": {},
   "source": [
    "## **Data Preprocessing For Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33f7b4",
   "metadata": {},
   "source": [
    "**Data Cleaning Is Not Data Preprocessing !**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1776c",
   "metadata": {},
   "source": [
    "It worthwhile to know that data cleaning is different from data preprocessing. In the machine learning implementation pipleline, data cleaning comes first. As matter of emphasis, throigh more light on data cleaning.\n",
    "\n",
    "The process of identifying andcorrecting errors, inconsistencies and inaccuracies in raw data is data cleaning. The tasks you would most likely encounter in data cleaning may include;\n",
    "* Handling missing values.\n",
    "* Removing duplicate rows or columns.\n",
    "* Correcting inconsistencies(like typos and inconsistent cases)\n",
    "* Handling outliers.\n",
    "* Handling corrupt strings(unicode problems).\n",
    "* Handling irrelivant data and more that you might have encountered or may likely encounter in your jourey with working with data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892d7dc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Other Conflicting Terms With \"Data Preprocessing\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9715f",
   "metadata": {},
   "source": [
    "It is quite important to note that as you progress in your learning, practice and research, you would come accross some big data lingo that might throw you in confusion, kind of wandering what they mean...some of those terms include **data mining**, **data wrangling**, **data preparation**, **data transformation**, **data harmonization**, **data refinement**, **data shaping**, d**ata manipulation**, **data manicuring**, **data validation** etc\n",
    "\n",
    "In your spare time, do well to take your time to learning their meaning, their similarities and difference with other term related terms.\n",
    "\n",
    "But two of those terms will be differentiated from data proprocessing in a simple table to get us going before moving into data preprocessing proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca44b6",
   "metadata": {},
   "source": [
    "| Feature       | Data Wrangling                                      | Data Preprocessing                               | Data Mining                                    |\n",
    "|--------------|---------------------------------------------------|-------------------------------------------------|-----------------------------------------------|\n",
    "| **Purpose**   | Preparing and structuring raw data               | Transforming data into a format suitable for ML | Extracting insights and patterns             |\n",
    "| **Focus**     | Cleaning, transforming, and integrating data     | Encoding, scaling, and feature selection       | Finding hidden relationships in data (the complete ML pipline)         |\n",
    "| **Techniques** | Data cleaning, merging, reshaping               | Normalization, encoding, feature extraction    | Clustering, classification, regression, anomaly detection |\n",
    "| **End Goal**  | Organized, structured data                       | Model ready dataset                            | Actionable insights for decision making      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2bad1",
   "metadata": {},
   "source": [
    "In the Nigerian setting, when you see people cooking many varieties of dishes with big sets of pots. The next thing that comes to mind is \"What is the celebration?\", that is, they are most likely cooking for a party or an event. It is the samething when it comes to data processing, So in a lay mans' definition, we can say that data preprocessing is the preparing and cooking of your dataset for ML model building. I think this should be clear enough.\n",
    "\n",
    "If simplicity feels like a scam to you, lets subscribe to a little bit of complexity in our definition.\n",
    "\n",
    "Data preprocessing is the step in the machine learning pipeline where raw data is transformed into a structured, clean, and suitable format for model training. It ensures that data is compatible with machine learning algorithms by handling inconsistencies, scaling, encoding, and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e3a2e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Why Should We Preprocess Data Before Building The ML Model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db0015",
   "metadata": {},
   "source": [
    "**NOTE**: *I will introduce some new terms here, don't get worried if you dont understand them yet, but is good you are aware of their existence. I will bolden then for emphasis*\n",
    "\n",
    "Lets break it down, the effectiveness of our model is at the mercy of how well our features are engineered, and **feature engineering** is at the mercy of data preprocesing. Data preprocessing is crucial in machine learning because raw data is often incomplete, inconsistent, and noisy.Without proper preprocessing, **ML algorithms** may learn incorrect patterns, leading to poor performance.\n",
    "\n",
    "\n",
    "So lets answer the question now.\n",
    "1. To improve data quality: After cleaning data **features** might still contain errors, incosistencies and missing features that can affect the performance of the **model**. So preprocesing prepares the dataset for **feature selection** and **feature extraction** helping.\n",
    "\n",
    "2. Enhance Model Performance: Most ML algorithms learns better when the features are  **scaled**, **normalized** and structured properly. So proper preprocessing ensures that features contribute meaningfully to **predictions**.\n",
    "\n",
    "3. Prevent Models From **Overfitting**, **Underfitting**, **High Variance** and **Bias**: Handling **imbalanced dataset**s prevents the model from being biased towards the dominant **class**. So proper preperocessing ensures that the features contributes meaningfully to predictions of the model.\n",
    "\n",
    "4. Ensures Algorithm Compatibility: Many ML algorithms require numeric inputs (e.g., **encoding** categorical variables).\n",
    "Some algorithms, like KNN and neural networks, are sensitive to scale, requiring normalization.\n",
    "\n",
    "5. Improves Model Accuracy And Efficiency: Well preprocessed data helps models learn better patterns, leading to improved **accuracy** and **generalization**.\n",
    "Reduces **computational complexity** by eliminating **irrelevant features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba90d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Technical Terms Used in Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70924924",
   "metadata": {},
   "source": [
    "| Term                        | Meaning                                                         | Implementation (Python)                                      | Use Case                                                      |\n",
    "|-----------------------------|-----------------------------------------------------------------|-------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| **Missing Value Imputation** | Strategies for filling in missing data                        | `df.fillna()` or `KNNImputer()` (from `sklearn`)             | Ensures complete datasets for analysis or model training      |\n",
    "| **Mean/Median/Mode Imputation** | Replacing missing values with the average, median, or mode    | `df.fillna(df.mean())`, `df.fillna(df.median())`             | Quick and simple imputation for numeric columns               |\n",
    "| **KNN Imputation**           | Using similar data points to fill in missing values            | `KNNImputer()` (from `sklearn`)                              | Used when data is missing at random and can be inferred        |\n",
    "| **Regression Imputation**    | Predicting missing values using a regression model            | Custom implementation using `LinearRegression()`             | Suitable for datasets with relationships between features     |\n",
    "| **Multiple Imputation**      | Creating multiple plausible versions of data and combining them | `IterativeImputer()` (from `sklearn`)                        | Better for uncertain data with missing values                 |\n",
    "| **Outlier Detection**        | Identifying and handling extreme values                        | `zscore()`, `IQR method`                                     | Prevents extreme values from distorting the model             |\n",
    "| **Z-score Method**           | Identifying outliers based on standard deviation               | `zscore(df['col'])`                                          | Used for identifying outliers in normally distributed data    |\n",
    "| **IQR Method**               | Identifying outliers based on interquartile range              | `IQR = df['col'].quantile(0.75) - df['col'].quantile(0.25)`  | Detects outliers in skewed or non-normal distributions        |\n",
    "| **Box Plots**                | Visual representation of data distribution with outliers      | `sns.boxplot(x='col', data=df)`                              | Helps visually identify outliers                             |\n",
    "| **Winsorizing/Clipping**     | Limiting extreme values to a specified threshold              | `winsorize()` from `scipy.stats`                             | Reduces the impact of outliers on model performance           |\n",
    "| **Data Deduplication**       | Removing duplicate records                                    | `df.drop_duplicates()`                                       | Prevents duplicate data from affecting analysis or models     |\n",
    "| **Noise Removal**            | Reducing random errors or noise in data                       | `smooth()` or `scipy.signal.savgol_filter()`                 | Improves signal-to-noise ratio in datasets                   |\n",
    "| **Smoothing Techniques**     | Applying filters to reduce random noise in time-series data   | `moving_average()`, `SavitzkyGolay()` (from `scipy`)         | Helps in smoothing time-series data or signals                |\n",
    "| **Data Type Conversion**     | Changing data types for consistency                           | `df['col'] = df['col'].astype(int)`                          | Ensures consistency in feature formats (e.g., string to int) |\n",
    "| **Data Transformation**      | Changing the format or structure of the data                  | `np.log()`, `BoxCox()` (from `scipy`)                        | Used for normalizing, stabilizing variance, or skewed data   |\n",
    "| **Normalization**            | Scaling data to a specific range (usually 0 to 1)             | `MinMaxScaler()` (from `sklearn`)                            | Common for algorithms requiring bounded features (e.g., KNN) |\n",
    "| **Min-Max Scaling**          | Scaling data between 0 and 1                                  | `MinMaxScaler().fit_transform(df)`                           | Ensures all features are in the same range                    |\n",
    "| **Standardization**          | Scaling data to have a mean of 0 and standard deviation of 1 | `StandardScaler()` (from `sklearn`)                          | Often required for algorithms like linear regression, SVM    |\n",
    "| **Robust Scaling**           | Scaling using median and IQR for robustness against outliers | `RobustScaler()` (from `sklearn`)                            | Handles datasets with many outliers                          |\n",
    "| **Feature Scaling**          | A broader term covering normalization and standardization    | Combination of `MinMaxScaler()`, `StandardScaler()`          | Ensures uniform scaling of features                           |\n",
    "| **Encoding Categorical Variables** | Converting categorical data into numeric format           | `pd.get_dummies()`, `LabelEncoder()`                         | Required for machine learning models to process categorical data |\n",
    "| **One-Hot Encoding**         | Creating binary columns for each category                     | `pd.get_dummies()`                                           | Used when categorical variables have no ordinal relationship  |\n",
    "| **Label Encoding**           | Assigning unique integers to categories                       | `LabelEncoder().fit_transform(df['col'])`                    | Used when categories have a natural order                    |\n",
    "| **Ordinal Encoding**         | Assigning integers to categories based on order               | Custom mapping: `{category: index}`                           | Suitable for ordinal data where order matters                |\n",
    "| **Target Encoding**          | Replacing categories with mean of the target variable         | `CategoryEncoders.TargetEncoder()`                           | Used in modeling when categories correlate with target variable |\n",
    "| **Discretization/Binning**   | Converting continuous variables into discrete bins            | `pd.cut()` or `KBinsDiscretizer()`                           | Used for reducing model complexity or creating categorical features |\n",
    "| **Log Transformation**       | Applying a log function to reduce skewness                    | `np.log(df['col'])`                                          | Used for positively skewed data or stabilizing variance       |\n",
    "| **Power Transformation**     | Box-Cox or Yeo-Johnson transformations for normalizing data  | `PowerTransformer()` (from `sklearn`)                        | Used for stabilizing variance and making data more Gaussian  |\n",
    "| **Polynomial Features**      | Creating new features by raising existing features to higher powers | `PolynomialFeatures()` (from `sklearn`)                      | Used for capturing non-linear relationships                   |\n",
    "| **Interaction Features**     | Creating new features by combining existing features         | `df['new_feature'] = df['feature1'] * df['feature2']`        | Used to capture interactions between features                 |\n",
    "| **Data Augmentation**        | Generating new data points based on existing data            | `ImageDataGenerator()` (from `Keras`), `augmented_text()`    | Common in image and text processing tasks                    |\n",
    "| **Data Integration**         | Merging data from multiple sources                           | `pd.merge()` or `pd.concat()`                               | Combines multiple datasets into a single source of truth     |\n",
    "| **Feature Selection**        | Choosing relevant features for model training                | `SelectKBest()`, `RFE()` (from `sklearn`)                   | Reduces dimensionality and avoids overfitting                |\n",
    "| **Filter Methods**           | Selecting features based on statistical measures             | `SelectKBest()` (from `sklearn`)                            | Selects features with the highest statistical relevance       |\n",
    "| **Wrapper Methods**          | Using a model to evaluate feature subsets                     | `RFE()` (Recursive Feature Elimination)                      | Evaluates feature sets by recursively fitting a model        |\n",
    "| **Embedded Methods**         | Built-in feature selection during model training            | `Lasso`, `DecisionTreeClassifier()`                          | Selects features while training models (e.g., L1 regularization) |\n",
    "| **Dimensionality Reduction** | Reducing the number of features while retaining key information | `PCA()`, `LDA()`, `t-SNE()`                                 | Reduces data complexity and overfitting risks                 |\n",
    "| **PCA (Principal Component Analysis)** | Finding principal components that capture data variance | `PCA()` (from `sklearn`)                                     | Used for feature extraction and noise reduction              |\n",
    "| **LDA (Linear Discriminant Analysis)** | Maximizing separability between classes                   | `LDA()` (from `sklearn`)                                     | Used for classification problems, especially in imbalanced data |\n",
    "| **t-SNE (t-Distributed Stochastic Neighbor Embedding)** | Reducing dimensionality while preserving structure | `TSNE()` (from `sklearn`)                                   | Used for visualizing high-dimensional data                   |\n",
    "| **Sampling**                 | Selecting a subset of data for processing                    | `train_test_split()`, `resample()`                          | Used when working with large datasets or for cross-validation |\n",
    "| **Data Serialization**       | Converting data to a suitable format for storage or transmission | `pickle`, `json`, `parquet`                                 | Ensures data can be stored and loaded efficiently             |\n",
    "| **JSON**                     | A lightweight data-interchange format                        | `json.load()` / `json.dump()`                               | Common format for storing and transmitting data              |\n",
    "| **XML**                      | A markup language for representing structured data           | `xml.etree.ElementTree`                                     | Used for structured data storage and exchange                |\n",
    "| **Pickle**                   | Python-specific format for serializing objects               | `pickle.dump()` / `pickle.load()`                           | Saves Python objects for future use or distributed computing  |\n",
    "| **Parquet**                  | A columnar storage format                                    | `pyarrow.parquet`                                           | Efficient storage format, commonly used in big data          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ec389",
   "metadata": {},
   "source": [
    "**Hands On Assigment**\n",
    "\n",
    "Study the table above;\n",
    "1. Tell why data cleaning and feature engineering seems to intertwine with data preprocessing. Give a list of terms that intertwined.\n",
    "2. Some terms here are familiar with some terms you encountered during your introduction to statistics class, can you identify them and their usecases when you were learning statistics?\n",
    "\n",
    "* **Hints:** Peer discussions are permitted, but ensure to have studied and understood some terms in the table before asking for help.*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Answer**\n",
    "1. Data cleaning and feature engineering do indeed intertwine with data preprocessing because the processes are overlapped across each section. For example, feature selection is used in data cleaning when working on a particular column, and it is also used in feature engineering when  you want to create a new feature (or column) from already existing columns.\n",
    "\n",
    "Some of the terms that are intertwined are:\n",
    "- Feature selection\n",
    "* Missing value imputation\n",
    "- Data transformation etc\n",
    "\n",
    "\n",
    "2. Scaling: This is the process of transforming numerical features that are skewed by scaling i.e. interpolating the number to make them fall between the range of 0 and 1. The two commonly used types are:\n",
    "**i** Normalization: This is used when the numerical feature is highly skewed, and the otliers are not negligible. The mathematical formula is (df['column'] - df['column'].min()) / (df['column'].max() - df['column'].min())\n",
    "\n",
    "\n",
    "**ii**  Standardization: This is used when the data is roughly skewed and outliers are negligible. Its uses the minimum and maximum values of the data. The formula is ((x- mean) / std)\n",
    "\n",
    "**iii** Measures of Center: These are used to determine the statistical value of a datasset, and they can also be used to fill in missing values for numerical and categorical data.\n",
    "\n",
    "**iv** Measures of spread: These are used to get and deal with outliers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f23e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the scikit-learn library is usually imported as \"sklearn\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71b182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58584ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign diabetes to load_diabetes()\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "#load the feature dataset\n",
    "features = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "#   Load the target dataset\n",
    "target = diabetes.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854213c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59eab16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019907 -0.017646  \n",
       "1 -0.039493 -0.068332 -0.092204  \n",
       "2 -0.002592  0.002861 -0.025930  \n",
       "3  0.034309  0.022688 -0.009362  \n",
       "4 -0.002592 -0.031988 -0.046641  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8eabfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b342029",
   "metadata": {},
   "source": [
    "#### **1. Handling  Missing Values**\n",
    "\n",
    "In most datasets,missing values might occur in features. You can handle them with imputation techniques such as mean or median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66bd4a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    0\n",
       "sex    0\n",
       "bmi    0\n",
       "bp     0\n",
       "s1     0\n",
       "s2     0\n",
       "s3     0\n",
       "s4     0\n",
       "s5     0\n",
       "s6     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "\n",
    "features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a08f5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     442 non-null    float64\n",
      " 1   sex     442 non-null    float64\n",
      " 2   bmi     442 non-null    float64\n",
      " 3   bp      442 non-null    float64\n",
      " 4   s1      442 non-null    float64\n",
      " 5   s2      442 non-null    float64\n",
      " 6   s3      442 non-null    float64\n",
      " 7   s4      442 non-null    float64\n",
      " 8   s5      442 non-null    float64\n",
      " 9   s6      442 non-null    float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 34.7 KB\n"
     ]
    }
   ],
   "source": [
    "# take a snapshot of the dataset\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d63a2e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286131, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04688253,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452873, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00422151,  0.00306441]], shape=(442, 10))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c3972",
   "metadata": {},
   "source": [
    "This dataset doesn't contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be63c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create missing values\n",
    "\n",
    "features.loc[10, 'age'] = None\n",
    "features.loc[50:56, 'bmi'] = None\n",
    "features.loc[100:105, 'bp'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e992300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    1\n",
       "sex    0\n",
       "bmi    7\n",
       "bp     6\n",
       "s1     0\n",
       "s2     0\n",
       "s3     0\n",
       "s4     0\n",
       "s5     0\n",
       "s6     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values againi\n",
    "features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8f07f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit function to transform the data\n",
    "\n",
    "# import the needed library\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the imputer class using \"mean\" as strategy for imputation\n",
    "impute_mean = SimpleImputer(strategy='mean')\n",
    "\n",
    "# create an instance of the imputer class using the median as strategy for imputation\n",
    "impute_median = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df2d9323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.age.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1804519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the defined instances above\n",
    "features['age'] = impute_mean.fit_transform(features[['age']])\n",
    "features['bmi'] = impute_median.fit_transform(features[['bmi']])\n",
    "features['bp'] = impute_mean.fit_transform(features[['bp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95f1fee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    0\n",
       "sex    0\n",
       "bmi    0\n",
       "bp     0\n",
       "s1     0\n",
       "s2     0\n",
       "s3     0\n",
       "s4     0\n",
       "s5     0\n",
       "s6     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d298b",
   "metadata": {},
   "source": [
    "I believe we should be familiar with outliers by now. Here, I will be introducing us to another method for handling outliers.\n",
    "\n",
    "Again, let me throw more light on handling outliers. Have you seen a family that everyone of them is tall? But there is this one child who is just \"heightly challenged\", I mean short. That child is an outlier.\n",
    "\n",
    "So outliers are data points that are significantly different from other observations in your dataset. They can be unusually high or low values. Think of them as the odd ones out that don't seem to fit the general pattern of the data.\n",
    "\n",
    "Outliers are not bad in themselves, for the purpose of data analysis or clustering or when building recommender systems or trying to detect anomaly they can be very useful...\n",
    "\n",
    "\n",
    "But when training a predictive model, they could introduce biases by assigning more weights to those higher values. And this would not help the performance of our model on new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c129cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79963488  1.06548848  1.30284864 ... -0.05449919  0.41853093\n",
      "  -0.37098854]\n",
      " [-0.04436617 -0.93853666 -1.08246065 ... -0.83030083 -1.43658851\n",
      "  -1.93847913]\n",
      " [ 1.79709067  1.06548848  0.93937293 ... -0.05449919  0.06015558\n",
      "  -0.54515416]\n",
      " ...\n",
      " [ 0.87636225  1.06548848 -0.33279202 ... -0.23293356 -0.98564884\n",
      "   0.32567395]\n",
      " [-0.96509458 -0.93853666  0.82578678 ...  0.55838411  0.93616291\n",
      "  -0.54515416]\n",
      " [-0.96509458 -0.93853666 -1.53680528 ... -0.83030083 -0.08875225\n",
      "   0.06442552]]\n"
     ]
    }
   ],
   "source": [
    "# Using Z-Score to get outliers\n",
    "z_scores = stats.zscore(features)       # Calculate the z-score for the whole dataframe\n",
    "\n",
    "# let's print the z-score\n",
    "print(z_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196b153",
   "metadata": {},
   "source": [
    "**Removal Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2675c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
